
import streamlit as st
import os
from langchain_groq import ChatGroq
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.prompts import PromptTemplate
from langchain_core.documents import Document
from langchain_core.output_parsers import StrOutputParser

st.set_page_config(page_title="RS InfoHub RAG", page_icon="ğŸ‡¬ğŸ‡ª", layout="centered")

# ========================
# API Key
# ========================
try:
    GROQ_API_KEY = st.secrets["GROQ_API_KEY"]
except Exception:
    st.error("GROQ_API_KEY áƒáƒ  áƒáƒ áƒ˜áƒ¡ áƒ“áƒáƒ§áƒ”áƒœáƒ”áƒ‘áƒ£áƒšáƒ˜ Secrets-áƒ¨áƒ˜!")
    st.stop()

# ========================
# 1. áƒ¤áƒáƒ˜áƒšáƒ”áƒ‘áƒ˜áƒ“áƒáƒœ áƒ¬áƒáƒ™áƒ˜áƒ—áƒ®áƒ•áƒ
# ========================
def load_documents_from_folder(folder_path: str = "docs") -> list[Document]:
    documents = []

    if not os.path.exists(folder_path):
        st.error(f"áƒ¡áƒáƒ¥áƒáƒ¦áƒáƒšáƒ“áƒ” '{folder_path}' áƒáƒ  áƒ›áƒáƒ˜áƒ«áƒ”áƒ‘áƒœáƒ!")
        st.stop()

    # áƒ›áƒ®áƒáƒšáƒáƒ“ .txt áƒ“áƒ .pdf â€” requirements.txt áƒ“áƒ áƒ¡áƒ®áƒ•áƒ áƒ¡áƒ”áƒ áƒ•áƒ˜áƒ¡ áƒ¤áƒáƒ˜áƒšáƒ”áƒ‘áƒ˜ áƒ˜áƒ’áƒœáƒáƒ áƒ˜áƒ áƒ“áƒ”áƒ‘áƒ
    files = [
        f for f in os.listdir(folder_path)
        if (f.endswith(".txt") or f.endswith(".pdf"))
        and f != "requirements.txt"
        and not f.startswith(".")
    ]

    if not files:
        st.error("docs/ áƒ¡áƒáƒ¥áƒáƒ¦áƒáƒšáƒ“áƒ”áƒ¨áƒ˜ áƒ“áƒáƒ™áƒ£áƒ›áƒ”áƒœáƒ¢áƒ”áƒ‘áƒ˜ áƒáƒ  áƒáƒ áƒ˜áƒ¡!")
        st.stop()

    for filename in sorted(files):
        filepath = os.path.join(folder_path, filename)
        try:
            if filename.endswith(".txt"):
                with open(filepath, "r", encoding="utf-8") as f:
                    content = f.read().strip()
            elif filename.endswith(".pdf"):
                try:
                    from pypdf import PdfReader
                    reader = PdfReader(filepath)
                    content = "\n\n".join(
                        p.extract_text() for p in reader.pages if p.extract_text()
                    ).strip()
                except ImportError:
                    st.warning(f"âš ï¸ {filename}: pypdf áƒ¡áƒáƒ­áƒ˜áƒ áƒáƒ PDF-áƒ˜áƒ¡áƒ—áƒ•áƒ˜áƒ¡")
                    continue

            if content:
                documents.append(Document(
                    page_content=content,
                    metadata={"source": filename}
                ))
        except Exception as e:
            st.warning(f"âš ï¸ {filename}: {e}")

    return documents

# ========================
# 2. BM25-áƒ¡áƒ¢áƒ˜áƒšáƒ˜áƒ¡ keyword retriever áƒ¥áƒáƒ áƒ—áƒ£áƒšáƒ˜áƒ¡áƒ—áƒ•áƒ˜áƒ¡
# ========================
def keyword_retrieve(query: str, documents: list[Document], top_k: int = 2) -> tuple[list[Document], bool]:
    """
    áƒ¥áƒáƒ áƒ—áƒ£áƒšáƒ˜ áƒ¢áƒ”áƒ¥áƒ¡áƒ¢áƒ˜áƒ¡áƒ—áƒ•áƒ˜áƒ¡ keyword-based retrieval.
    áƒáƒ‘áƒ áƒ£áƒœáƒ”áƒ‘áƒ¡: (áƒ“áƒáƒ™áƒ£áƒ›áƒ”áƒœáƒ¢áƒ”áƒ‘áƒ˜áƒ¡ áƒ¡áƒ˜áƒ, found: bool)
    """
    query_lower = query.lower()
    query_words = set(query_lower.split())
    query_words = {w for w in query_words if len(w) > 2}

    scored = []
    for doc in documents:
        content_lower = doc.page_content.lower()
        score = 0

        # áƒ¡áƒ áƒ£áƒšáƒ˜ áƒ¡áƒ˜áƒ¢áƒ§áƒ•áƒ”áƒ‘áƒ˜áƒ¡ áƒ¨áƒ”áƒ›áƒáƒ¬áƒ›áƒ”áƒ‘áƒ
        score += sum(2 for w in query_words if w in content_lower)

        # áƒáƒ¡áƒ”áƒ•áƒ” áƒ•áƒáƒ›áƒáƒ¬áƒ›áƒ”áƒ‘áƒ— áƒœáƒáƒ¬áƒ˜áƒšáƒáƒ‘áƒ áƒ˜áƒ• áƒ“áƒáƒ›áƒ—áƒ®áƒ•áƒ”áƒ•áƒáƒ¡ (áƒ¥áƒáƒ áƒ—áƒ£áƒšáƒ˜ áƒ¤áƒšáƒ”áƒ¥áƒ¡áƒ˜áƒ)
        # áƒ›áƒáƒ’: "áƒ¡áƒ¢áƒ£áƒ“áƒ”áƒœáƒ¢áƒ˜" áƒ›áƒáƒ˜áƒ«áƒ”áƒ‘áƒœáƒ”áƒ‘áƒ "áƒ¡áƒ¢áƒ£áƒ“áƒ”áƒœáƒ¢"-áƒ¨áƒ˜
        for w in query_words:
            if len(w) > 4:
                stem = w[:len(w)-2]  # áƒ¡áƒ˜áƒ¢áƒ§áƒ•áƒ˜áƒ¡ áƒ¤áƒ£áƒ«áƒ” (áƒ›áƒáƒ áƒ¢áƒ˜áƒ•áƒ˜)
                score += sum(1 for _ in [1] if stem in content_lower)

        scored.append((score, doc))

    scored.sort(key=lambda x: x[0], reverse=True)
    best_score = scored[0][0] if scored else 0

    # áƒ—áƒ£ áƒ¡áƒáƒ£áƒ™áƒ”áƒ—áƒ”áƒ¡áƒ score áƒ«áƒáƒšáƒ˜áƒáƒœ áƒ“áƒáƒ‘áƒáƒšáƒ˜áƒ â€” áƒ˜áƒœáƒ¤áƒáƒ áƒ›áƒáƒªáƒ˜áƒ áƒ‘áƒáƒ–áƒáƒ¨áƒ˜ áƒáƒ  áƒáƒ áƒ˜áƒ¡
    if best_score < 1:
        return [], False

    matched = [(s, d) for s, d in scored if s > 0]
    return [d for _, d in matched[:top_k]], True

# ========================
# 3. Chunking + Retrieval
# ========================
@st.cache_resource
def setup_chunks() -> list[Document]:
    raw_docs = load_documents_from_folder("docs")
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=600,
        chunk_overlap=60,
        separators=["\n\n", "\n", ".", " "]
    )
    return splitter.split_documents(raw_docs), raw_docs

@st.cache_resource
def get_llm(_api_key: str):
    return ChatGroq(
        api_key=_api_key,
        model_name="llama-3.3-70b-versatile",
        temperature=0,
        max_tokens=1024,
    )

# ========================
# 4. áƒáƒáƒ¡áƒ£áƒ®áƒ˜áƒ¡ áƒ’áƒ”áƒœáƒ”áƒ áƒáƒªáƒ˜áƒ
# ========================
def get_answer(query: str, chunks: list[Document], llm) -> tuple[str, list[Document], bool]:
    retrieved, found = keyword_retrieve(query, chunks, top_k=3)

    # áƒ—áƒ£ áƒ¡áƒáƒ”áƒ áƒ—áƒáƒ“ áƒáƒ  áƒ›áƒáƒ˜áƒ«áƒ”áƒ‘áƒœáƒ áƒ¨áƒ”áƒ¡áƒáƒ‘áƒáƒ›áƒ˜áƒ¡áƒ˜ chunk
    if not found:
        return "âŒ áƒáƒ› áƒ™áƒ˜áƒ—áƒ®áƒ•áƒáƒ–áƒ” áƒ˜áƒœáƒ¤áƒáƒ áƒ›áƒáƒªáƒ˜áƒ áƒ‘áƒáƒ–áƒáƒ¨áƒ˜ áƒáƒ  áƒ›áƒáƒ˜áƒáƒáƒ•áƒ”áƒ‘áƒ.", [], False

    context = "\n\n---\n\n".join(
        f"[áƒ¬áƒ§áƒáƒ áƒ: {d.metadata['source']}]\n{d.page_content}"
        for d in retrieved
    )

    prompt = PromptTemplate.from_template(
        "áƒ¨áƒ”áƒœ áƒ®áƒáƒ  áƒ¡áƒáƒ’áƒáƒ“áƒáƒ¡áƒáƒ®áƒáƒ“áƒ/áƒ¡áƒáƒ‘áƒáƒŸáƒ áƒáƒ¡áƒ˜áƒ¡áƒ¢áƒ”áƒœáƒ¢áƒ˜ RS InfoHub-áƒ˜áƒ¡áƒ—áƒ•áƒ˜áƒ¡.\n"
        "áƒ£áƒáƒáƒ¡áƒ£áƒ®áƒ” áƒ™áƒ˜áƒ—áƒ®áƒ•áƒáƒ¡ áƒ›áƒ®áƒáƒšáƒáƒ“ áƒ¥áƒ•áƒ”áƒ›áƒáƒ— áƒ›áƒáƒªáƒ”áƒ›áƒ£áƒš áƒ™áƒáƒœáƒ¢áƒ”áƒ¥áƒ¡áƒ¢áƒ–áƒ” áƒ“áƒáƒ§áƒ áƒ“áƒœáƒáƒ‘áƒ˜áƒ— áƒ¥áƒáƒ áƒ—áƒ£áƒš áƒ”áƒœáƒáƒ–áƒ”.\n"
        "áƒ—áƒ£ áƒ™áƒáƒœáƒ¢áƒ”áƒ¥áƒ¡áƒ¢áƒ¨áƒ˜ áƒ™áƒ˜áƒ—áƒ®áƒ•áƒáƒ–áƒ” áƒáƒáƒ¡áƒ£áƒ®áƒ˜ áƒáƒ  áƒáƒ áƒ˜áƒ¡, áƒ“áƒáƒ¬áƒ”áƒ áƒ” áƒ›áƒ®áƒáƒšáƒáƒ“: NO_INFO\n"
        "áƒ¡áƒ®áƒ•áƒ áƒ¨áƒ”áƒ›áƒ—áƒ®áƒ•áƒ”áƒ•áƒáƒ¨áƒ˜ áƒáƒáƒ¡áƒ£áƒ®áƒ˜ áƒ’áƒáƒ¡áƒªáƒ” áƒ“áƒ áƒ‘áƒáƒšáƒáƒ¨áƒ˜ áƒ›áƒ˜áƒ£áƒ—áƒ˜áƒ—áƒ”:\n"
        "áƒ¬áƒ§áƒáƒ áƒ: [áƒ¤áƒáƒ˜áƒšáƒ˜áƒ¡ áƒ¡áƒáƒ®áƒ”áƒšáƒ˜]\n"
        "áƒáƒáƒ¡áƒ£áƒ®áƒ˜ áƒ›áƒáƒ›áƒ–áƒáƒ“áƒ”áƒ‘áƒ£áƒšáƒ˜áƒ RS InfoHub-áƒ˜áƒ¡ áƒ›áƒ˜áƒ®áƒ”áƒ“áƒ•áƒ˜áƒ— - https://infohub.rs.ge/ka\n\n"
        "áƒ™áƒáƒœáƒ¢áƒ”áƒ¥áƒ¡áƒ¢áƒ˜:\n{context}\n\n"
        "áƒ™áƒ˜áƒ—áƒ®áƒ•áƒ: {question}\n\náƒáƒáƒ¡áƒ£áƒ®áƒ˜:"
    )

    chain = prompt | llm | StrOutputParser()
    answer = chain.invoke({"context": context, "question": query})

    # áƒ—áƒ£ LLM-áƒ›áƒ áƒ—áƒ¥áƒ•áƒ NO_INFO â€” áƒ¬áƒ§áƒáƒ áƒ áƒáƒ  áƒ’áƒáƒ›áƒáƒ©áƒœáƒ“áƒ”áƒ¡
    if "NO_INFO" in answer:
        return "âŒ áƒáƒ› áƒ™áƒ˜áƒ—áƒ®áƒ•áƒáƒ–áƒ” áƒ˜áƒœáƒ¤áƒáƒ áƒ›áƒáƒªáƒ˜áƒ áƒ‘áƒáƒ–áƒáƒ¨áƒ˜ áƒáƒ  áƒ›áƒáƒ˜áƒáƒáƒ•áƒ”áƒ‘áƒ.", [], False

    return answer, retrieved, True

# ========================
# 5. UI
# ========================
st.title("ğŸ‡¬ğŸ‡ª RS InfoHub â€” RAG áƒáƒ’áƒ”áƒœáƒ¢áƒ˜")
st.caption("áƒ¡áƒáƒ’áƒáƒ“áƒáƒ¡áƒáƒ®áƒáƒ“áƒ áƒ“áƒ áƒ¡áƒáƒ‘áƒáƒŸáƒ áƒ™áƒ˜áƒ—áƒ®áƒ•áƒ”áƒ‘áƒ–áƒ” áƒáƒáƒ¡áƒ£áƒ®áƒ˜ docs/ áƒ¡áƒáƒ¥áƒáƒ¦áƒáƒšáƒ“áƒ˜áƒ¡ áƒ“áƒáƒ™áƒ£áƒ›áƒ”áƒœáƒ¢áƒ”áƒ‘áƒ˜áƒ¡ áƒ¡áƒáƒ¤áƒ£áƒ«áƒ•áƒ”áƒšáƒ–áƒ”")

with st.spinner("áƒ“áƒáƒ™áƒ£áƒ›áƒ”áƒœáƒ¢áƒ”áƒ‘áƒ˜ áƒ˜áƒ¢áƒ•áƒ˜áƒ áƒ—áƒ”áƒ‘áƒ..."):
    chunks, raw_docs = setup_chunks()
    llm = get_llm(GROQ_API_KEY)

st.success(f"âœ… {len(raw_docs)} áƒ“áƒáƒ™áƒ£áƒ›áƒ”áƒœáƒ¢áƒ˜ áƒ©áƒáƒ˜áƒ¢áƒ•áƒ˜áƒ áƒ—áƒ â†’ {len(chunks)} chunk-áƒáƒ“ áƒ“áƒáƒ˜áƒ§áƒ")

with st.expander("ğŸ“‚ áƒ©áƒáƒ¢áƒ•áƒ˜áƒ áƒ—áƒ£áƒšáƒ˜ áƒ“áƒáƒ™áƒ£áƒ›áƒ”áƒœáƒ¢áƒ”áƒ‘áƒ˜"):
    docs_folder = "docs"
    if os.path.exists(docs_folder):
        for f in sorted(os.listdir(docs_folder)):
            if (f.endswith(".txt") or f.endswith(".pdf")) and f != "requirements.txt":
                size = os.path.getsize(os.path.join(docs_folder, f))
                icon = "ğŸ“„" if f.endswith(".txt") else "ğŸ“•"
                st.markdown(f"- {icon} `{f}` â€” {size:,} byte")

st.divider()

if "messages" not in st.session_state:
    st.session_state.messages = []

for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

user_query = st.chat_input("áƒ“áƒáƒ¡áƒ•áƒ˜ áƒ™áƒ˜áƒ—áƒ®áƒ•áƒ áƒ¥áƒáƒ áƒ—áƒ£áƒšáƒáƒ“...")

if user_query:
    st.session_state.messages.append({"role": "user", "content": user_query})
    with st.chat_message("user"):
        st.markdown(user_query)

    with st.chat_message("assistant"):
        try:
            with st.spinner("áƒáƒáƒ¡áƒ£áƒ®áƒ˜ áƒ˜áƒ«áƒ”áƒ‘áƒœáƒ”áƒ‘áƒ..."):
                answer, source_docs, found = get_answer(user_query, chunks, llm)

            st.markdown(answer)

            # chunk-áƒ”áƒ‘áƒ˜ áƒ›áƒ®áƒáƒšáƒáƒ“ áƒ›áƒáƒ¨áƒ˜áƒœ áƒ©áƒáƒœáƒ¡ áƒ áƒáƒªáƒ áƒ˜áƒœáƒ¤áƒáƒ áƒ›áƒáƒªáƒ˜áƒ áƒ›áƒáƒ˜áƒ«áƒ”áƒ‘áƒœáƒ
            if found and source_docs:
                with st.expander("ğŸ” áƒ’áƒáƒ›áƒáƒ§áƒ”áƒœáƒ”áƒ‘áƒ£áƒšáƒ˜ Chunk-áƒ”áƒ‘áƒ˜"):
                    for i, doc in enumerate(source_docs, 1):
                        st.markdown(f"**Chunk {i} â€” {doc.metadata['source']}**")
                        st.caption(doc.page_content[:400] + ("..." if len(doc.page_content) > 400 else ""))

        except Exception as e:
            st.error(f"áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ: {str(e)}")
            answer = "áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ áƒ›áƒáƒ®áƒ“áƒ."

    st.session_state.messages.append({"role": "assistant", "content": answer})
