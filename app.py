import streamlit as st
import os
from langchain_groq import ChatGroq
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.prompts import PromptTemplate
from langchain_core.documents import Document
from langchain_core.output_parsers import StrOutputParser

st.set_page_config(page_title="RS InfoHub RAG", page_icon="ğŸ‡¬ğŸ‡ª", layout="centered")

# ========================
# API Key
# ========================
try:
    GROQ_API_KEY = st.secrets["GROQ_API_KEY"]
except Exception:
    st.error("GROQ_API_KEY áƒáƒ  áƒáƒ áƒ˜áƒ¡ áƒ“áƒáƒ§áƒ”áƒœáƒ”áƒ‘áƒ£áƒšáƒ˜ Secrets-áƒ¨áƒ˜!")
    st.stop()

# ========================
# 1. áƒ¤áƒáƒ˜áƒšáƒ”áƒ‘áƒ˜áƒ“áƒáƒœ áƒ¬áƒáƒ™áƒ˜áƒ—áƒ®áƒ•áƒ (TXT + PDF)
# ========================
def load_documents_from_folder(folder_path: str = "docs") -> list[Document]:
    documents = []

    if not os.path.exists(folder_path):
        st.error(f"áƒ¡áƒáƒ¥áƒáƒ¦áƒáƒšáƒ“áƒ” '{folder_path}' áƒáƒ  áƒ›áƒáƒ˜áƒ«áƒ”áƒ‘áƒœáƒ!")
        st.stop()

    files = [f for f in os.listdir(folder_path) if f.endswith(".txt") or f.endswith(".pdf")]

    if not files:
        st.error("docs/ áƒ¡áƒáƒ¥áƒáƒ¦áƒáƒšáƒ“áƒ”áƒ¨áƒ˜ .txt áƒáƒœ .pdf áƒ¤áƒáƒ˜áƒšáƒ”áƒ‘áƒ˜ áƒáƒ  áƒáƒ áƒ˜áƒ¡!")
        st.stop()

    for filename in sorted(files):
        filepath = os.path.join(folder_path, filename)

        try:
            if filename.endswith(".txt"):
                with open(filepath, "r", encoding="utf-8") as f:
                    content = f.read().strip()

            elif filename.endswith(".pdf"):
                try:
                    from pypdf import PdfReader
                    reader = PdfReader(filepath)
                    content = "\n\n".join(
                        page.extract_text() for page in reader.pages
                        if page.extract_text()
                    ).strip()
                except ImportError:
                    st.warning(f"âš ï¸ {filename}: pypdf áƒáƒ  áƒáƒ áƒ˜áƒ¡ áƒ“áƒáƒ˜áƒœáƒ¡áƒ¢áƒáƒšáƒ˜áƒ áƒ”áƒ‘áƒ£áƒšáƒ˜. áƒ’áƒáƒ›áƒáƒ˜áƒ§áƒ”áƒœáƒ” .txt áƒ¤áƒáƒ˜áƒšáƒ”áƒ‘áƒ˜.")
                    continue

            if content:
                documents.append(Document(
                    page_content=content,
                    metadata={"source": filename}
                ))

        except Exception as e:
            st.warning(f"âš ï¸ {filename} áƒ•áƒ”áƒ  áƒ¬áƒáƒ˜áƒ™áƒ˜áƒ—áƒ®áƒ: {e}")

    return documents

# ========================
# 2. RAG áƒ¡áƒ˜áƒ¡áƒ¢áƒ”áƒ›áƒ
# ========================
@st.cache_resource
def setup_rag(_api_key: str):
    raw_docs = load_documents_from_folder("docs")

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=50,
        separators=["\n\n", "\n", ".", " "]
    )
    split_docs = splitter.split_documents(raw_docs)

    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    )

    vectorstore = FAISS.from_documents(split_docs, embeddings)

    retriever = vectorstore.as_retriever(
        search_type="mmr",
        search_kwargs={"k": 3, "fetch_k": 10}
    )

    llm = ChatGroq(
        api_key=_api_key,
        model_name="llama-3.3-70b-versatile",
        temperature=0,
        max_tokens=1024,
    )

    return retriever, llm, len(split_docs), len(raw_docs)

# ========================
# 3. áƒáƒáƒ¡áƒ£áƒ®áƒ˜áƒ¡ áƒ’áƒ”áƒœáƒ”áƒ áƒáƒªáƒ˜áƒ
# ========================
def get_answer(query: str, retriever, llm) -> tuple[str, list[Document]]:
    retrieved_docs = retriever.invoke(query)

    context = "\n\n---\n\n".join(
        f"[áƒ¬áƒ§áƒáƒ áƒ: {d.metadata['source']}]\n{d.page_content}"
        for d in retrieved_docs
    )

    prompt = PromptTemplate.from_template(
        "áƒ¨áƒ”áƒœ áƒ®áƒáƒ  áƒ¡áƒáƒ’áƒáƒ“áƒáƒ¡áƒáƒ®áƒáƒ“áƒ/áƒ¡áƒáƒ‘áƒáƒŸáƒ áƒáƒ¡áƒ˜áƒ¡áƒ¢áƒ”áƒœáƒ¢áƒ˜ RS InfoHub-áƒ˜áƒ¡áƒ—áƒ•áƒ˜áƒ¡.\n"
        "áƒ£áƒáƒáƒ¡áƒ£áƒ®áƒ” áƒ™áƒ˜áƒ—áƒ®áƒ•áƒáƒ¡ áƒ›áƒ®áƒáƒšáƒáƒ“ áƒ¥áƒ•áƒ”áƒ›áƒáƒ— áƒ›áƒáƒªáƒ”áƒ›áƒ£áƒš áƒ™áƒáƒœáƒ¢áƒ”áƒ¥áƒ¡áƒ¢áƒ–áƒ” áƒ“áƒáƒ§áƒ áƒ“áƒœáƒáƒ‘áƒ˜áƒ— áƒ¥áƒáƒ áƒ—áƒ£áƒš áƒ”áƒœáƒáƒ–áƒ”.\n"
        "áƒ—áƒ£ áƒ™áƒáƒœáƒ¢áƒ”áƒ¥áƒ¡áƒ¢áƒ¨áƒ˜ áƒáƒáƒ¡áƒ£áƒ®áƒ˜ áƒáƒ  áƒáƒ áƒ˜áƒ¡, áƒ—áƒ¥áƒ•áƒ˜: 'áƒáƒ› áƒ™áƒ˜áƒ—áƒ®áƒ•áƒáƒ–áƒ” áƒ˜áƒœáƒ¤áƒáƒ áƒ›áƒáƒªáƒ˜áƒ áƒ‘áƒáƒ–áƒáƒ¨áƒ˜ áƒáƒ  áƒ›áƒáƒ˜áƒáƒáƒ•áƒ”áƒ‘áƒ.'\n\n"
        "áƒ™áƒáƒœáƒ¢áƒ”áƒ¥áƒ¡áƒ¢áƒ˜:\n{context}\n\n"
        "áƒ™áƒ˜áƒ—áƒ®áƒ•áƒ: {question}\n\n"
        "áƒáƒáƒ¡áƒ£áƒ®áƒ˜ (áƒ‘áƒáƒšáƒáƒ¨áƒ˜ áƒ›áƒ˜áƒ£áƒ—áƒ˜áƒ—áƒ” áƒ¬áƒ§áƒáƒ áƒ áƒ“áƒ: "
        "áƒáƒáƒ¡áƒ£áƒ®áƒ˜ áƒ›áƒáƒ›áƒ–áƒáƒ“áƒ”áƒ‘áƒ£áƒšáƒ˜áƒ RS InfoHub-áƒ˜áƒ¡ áƒ›áƒ˜áƒ®áƒ”áƒ“áƒ•áƒ˜áƒ— - https://infohub.rs.ge/ka):"
    )

    chain = prompt | llm | StrOutputParser()
    answer = chain.invoke({"context": context, "question": query})
    return answer, retrieved_docs

# ========================
# 4. UI
# ========================
st.title("ğŸ‡¬ğŸ‡ª RS InfoHub â€” RAG áƒáƒ’áƒ”áƒœáƒ¢áƒ˜")
st.caption("áƒ¡áƒáƒ’áƒáƒ“áƒáƒ¡áƒáƒ®áƒáƒ“áƒ áƒ“áƒ áƒ¡áƒáƒ‘áƒáƒŸáƒ áƒ™áƒ˜áƒ—áƒ®áƒ•áƒ”áƒ‘áƒ–áƒ” áƒáƒáƒ¡áƒ£áƒ®áƒ˜ docs/ áƒ¡áƒáƒ¥áƒáƒ¦áƒáƒšáƒ“áƒ˜áƒ¡ áƒ“áƒáƒ™áƒ£áƒ›áƒ”áƒœáƒ¢áƒ”áƒ‘áƒ˜áƒ¡ áƒ¡áƒáƒ¤áƒ£áƒ«áƒ•áƒ”áƒšáƒ–áƒ”")

with st.spinner("áƒ“áƒáƒ™áƒ£áƒ›áƒ”áƒœáƒ¢áƒ”áƒ‘áƒ˜ áƒ˜áƒ¢áƒ•áƒ˜áƒ áƒ—áƒ”áƒ‘áƒ..."):
    retriever, llm, chunk_count, doc_count = setup_rag(GROQ_API_KEY)

st.success(f"âœ… {doc_count} áƒ“áƒáƒ™áƒ£áƒ›áƒ”áƒœáƒ¢áƒ˜ áƒ©áƒáƒ˜áƒ¢áƒ•áƒ˜áƒ áƒ—áƒ â†’ {chunk_count} chunk-áƒáƒ“ áƒ“áƒáƒ˜áƒ§áƒ")

with st.expander("ğŸ“‚ áƒ©áƒáƒ¢áƒ•áƒ˜áƒ áƒ—áƒ£áƒšáƒ˜ áƒ“áƒáƒ™áƒ£áƒ›áƒ”áƒœáƒ¢áƒ”áƒ‘áƒ˜"):
    docs_folder = "docs"
    if os.path.exists(docs_folder):
        for f in sorted(os.listdir(docs_folder)):
            if f.endswith((".txt", ".pdf")):
                size = os.path.getsize(os.path.join(docs_folder, f))
                icon = "ğŸ“„" if f.endswith(".txt") else "ğŸ“•"
                st.markdown(f"- {icon} `{f}` â€” {size:,} byte")

st.divider()

if "messages" not in st.session_state:
    st.session_state.messages = []

for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

user_query = st.chat_input("áƒ“áƒáƒ¡áƒ•áƒ˜ áƒ™áƒ˜áƒ—áƒ®áƒ•áƒ áƒ¥áƒáƒ áƒ—áƒ£áƒšáƒáƒ“...")

if user_query:
    st.session_state.messages.append({"role": "user", "content": user_query})
    with st.chat_message("user"):
        st.markdown(user_query)

    with st.chat_message("assistant"):
        try:
            with st.spinner("áƒáƒáƒ¡áƒ£áƒ®áƒ˜ áƒ˜áƒ«áƒ”áƒ‘áƒœáƒ”áƒ‘áƒ..."):
                answer, source_docs = get_answer(user_query, retriever, llm)

            st.markdown(answer)

            with st.expander("ğŸ” áƒ’áƒáƒ›áƒáƒ§áƒ”áƒœáƒ”áƒ‘áƒ£áƒšáƒ˜ Chunk-áƒ”áƒ‘áƒ˜"):
                for i, doc in enumerate(source_docs, 1):
                    st.markdown(f"**Chunk {i} â€” {doc.metadata['source']}**")
                    st.caption(doc.page_content[:400] + ("..." if len(doc.page_content) > 400 else ""))

        except Exception as e:
            st.error(f"áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ: {str(e)}")
            answer = "áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ áƒ›áƒáƒ®áƒ“áƒ."

    st.session_state.messages.append({"role": "assistant", "content": answer})
